---
papersize: a4
documentclass: article
title: Analyzing the Yelp dataset
author: Michael Backs and Daniel Baron
date: \today
abstract: |
	Yelp is a web platform to collect and present user ratings and suggestions for restaurants and other locations such as bars, clubs and various other places. Yelp releases a dump of their dataset once a year to allow students to analyze it. This paper is about analyzing the Yelp dataset to find funny reviews and classying them into six funniness categories using supervised classification.
bibliography: references.bib
classoption:
- twocolumn
...

# Introduction

Text classification is applied in a number of fields and situations. @jurafski_martin_3rd give marketing or politics as an example for fields that wish to classify a text or product as positive or negative. They also say that text classification is often used to identify spam emails or find the language a given text is written in (ibid). While sentiment analysis and spam detection tasks use binary classification, we will pursue a multiple classification approach in our project. Our aim is to predict a 'funniness' category for a given review realeased on the web platform Yelp. These reviews have a number of funny votes ranging from zero to 100 and more. Assuming that the number of funny votes reflects how funny a text is, we will train a model to classify a given text in terms of funniness. I.e. predicting the degree of funniness. A problem arises with reviews having zero funny votes. This could either mean that those reviews in questions were not considered funny by the userbase or that just too few people or no people at all read the review, so that they could still be funny. We will tackle this issue with a condition described in the analysis section.

@iacobdeniz used the same dataset to inspect a similar aspect in their Kaggle Notebook. They are splitting the reviews into positive and negative ones and train a decision tree to figure out if a review is rather positive or negative. By leaving out 3-star-reviews, they don't use any neutral reviews: "It would make sense to associate 4- and 5-star reviews with a positive sentiment and 1- and 2-star reviews with a negative sentiment. 3-star reviews would be neutral, but for simplicity purposes, we will only attempt to predict the positive and negative sentiment, and we will revisit neutral later." @iacobdeniz


# Dataset


The semi structured dataset taken from Kaggle consists of five JSON files including reviewed businesses, a record of dates and locations when reviewers were at a business, the actual user reviews, a list of tips given by users and information on users who wrote reviews. 
Since our focus lies on text classification we only use the file containing users' reviews totalling roughly 5.2 million reviews. The other files help us reduce the amount of reviews to a managable size because they contain additional information such as in which state a review was made. This allowed grouping reviews by states of the USA. Furthermore, each review is rated how 'funny', 'cool' and 'helpful' it is by the Yelp user base. For instance, a given review might be rated 'funny' three times, 'cool' zero times and 'helpful' five times. But it could also have zero ratings for all three categories. 

This raises the problem of how much of our data has the correct label which is needed for supervised learning. Our first suggestion is that reviews that were not rated at all are not labeled. Thus, reviews must at least have one vote in at least one of the three categories to be accepted as labeled data. An alternative would be considering all of our data as labeled, risking that reviews that have not been read at all are given a false label.

Putting such issues aside for the moment, we limit our first analysis to reviews given for all Illinois-based locations present in the Yelp Dataset, which results in 42,316 reviews for 1,930 businesses. To improve classification training results, further data can be taken into account by other states.

The reduced dataset was created by preprocessing the businesses file consisting of 192,609 businesses and feeding latitude and longitude coordinates into the reverse geocoder python library written by @reversegeocoder to get a more accurate state relation since we found the adresses and states not to match up in every case.

When we follow our first suggestion, our extracted 42,316 reviews for the state of Illinois yield us 23,946 labeled reviews and 18,370 unlabeled ones. If we take the different approach all reviews are considered as labeled. 
Stop words are removed by deleting the 50 most frequent words except from content words such as 'service' so that classficatinal performance with and without stop words can be observed.

(The dataset is already pre-classified by the Yelp user database, so individuals had the chance to vote for a review to be funny. 7.763 out of the extracted 42.316 reviews were rated funny at least once, which leaves 34.553 reviews up for classification.)

Moreover, it is undoubtedly clear that the reviews' votings are subjective. Thus the ratings lack any formal or tracable criteria according to which they were rated funny. 



# Methodology

As a baseline we will use a counting vector to represent each review's features to train a naive Bayes classifier. Alternatively an even easier feature representation can be employed which is extracting negative and positive statements.

The precticted labels are funniness categories one to four: The first category means that a review is not funny, the second category indicates that it is slightly funny, while the third and the fourth imply that a given review is quite funny or very funny respectively.

It is undoubtedly clear that the reviews we work with are subjective. Thus the ratings lack any formal or tracable criteria by which a review received a funny rate. We assume that zero funny votes mean that a review was not deemed funny by the majority of the Yelp userbase. Surely, a zero rated review could still be funny because users just might not have cared to rate it funny or too few users read the review in question. 

In order to improve the result that is given by the baseline classifyer model, we will apply word embeddings which take into account distribution and context of words in contrast to counting vectors. The letter only work with a word's frequency. Word2vec might also be tested because short and dense vectors might yield better results. Moreover, we will draw opon more data to further improve results.


# Analysis


## Preliminary matters

As mentioned in the introduction, it is questionable how much of our data bears the correct label. This is way we suggest a condition that sorts out reviews that are likely to not have been read and therefore do not have correct labels, i.e. votes reflecting the users' impression of how amusing a review is. If a review is rated at least funny one time it can be used for training the classifier model. If a review is not rated funny, it must at least be rated as cool or useful to be accepted as training data. Training the classifier will be done with and without this condition.

![diagram 1](projectseminar.png){width=100%}

## Categories

We will utilize multiple mutually exclusive classification alongside supervised learning. The classes in our data are basically the number of funny votes. We propose reducing this high number to six classes each reflecting different funniess categories.


## Baseline

Now we use Naive Bayes as our baseline classifier, which is a probabilistic classifier based on Bayes' rule that makes two simplifying assumptions. Firstly, the position and distribution of words do not matter, which is known as bag-of-words method [@jurafski_martin_3rd, chapter 4 page 4]. Secondly, the probabilities for each feature of a given text are independent and thus simply multiplied (ibid.). A feature of given text is a word.

We represent each text as a sparse text vector. It is counted how many times each word type of the overall vocabulary occurs in a given text. Word types that do not occur in a given text are represented by zeros. Here, a given text is a set of features $f_{1}$, $f_{2}$, ... ,$f_{n}$ element of F. The classifier then predicts a class c for a review by calculating the probability for each class and choosing the one with the highest probability.  

$$c_{NB} = \underset{c\in C}{\operatorname{argmax} } P(c) \prod_{f\in F} P(f|c)$$



# Conclusion
In a nutshell, we're done.


# References

\footnotesize
